{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import backtrader as bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCSVData(bt.feeds.GenericCSVData):\n",
    "    params = (\n",
    "        ('nullvalue', float('nan')),\n",
    "        ('dtformat', '%Y-%m-%d'),\n",
    "        ('tmformat', '%H:%M:%S'),\n",
    "        ('datetime', 0),\n",
    "        ('open', None),\n",
    "        ('high', None),\n",
    "        ('low', None),\n",
    "        ('close', None),\n",
    "        ('volume', None),\n",
    "    )\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCSVData, self).__init__(*args, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, file_path, *args, **kwargs):\n",
    "        \"\"\"Load CSV file and dynamically adjust parameters based on file type.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "        # Read a few rows to determine the format\n",
    "        sample_data = pd.read_csv(file_path, nrows=5)\n",
    "\n",
    "        # Adjust parameters based on column names\n",
    "        params = {}\n",
    "        if {'Date', 'Open', 'High', 'Low', 'Close', 'Volume'}.issubset(sample_data.columns):\n",
    "            params.update({'datetime': 0, 'open': 1, 'high': 2, 'low': 3, 'close': 4, 'volume': 5})\n",
    "        elif {'open', 'close', 'high', 'low', 'volume'}.issubset(sample_data.columns):\n",
    "            params.update({'datetime': 0, 'open': 1, 'high': 3, 'low': 4, 'close': 2, 'volume': 5})\n",
    "        elif {'Date', 'Hour_of_Day', 'Close'}.issubset(sample_data.columns):\n",
    "            params.update({'datetime': 0, 'open': None, 'high': None, 'low': None, 'close': 2, 'volume': None})\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported CSV format: {file_path}\")\n",
    "\n",
    "        # Update class parameters\n",
    "        kwargs.update(params)\n",
    "\n",
    "        # Sort data by date\n",
    "        data = pd.read_csv(file_path)\n",
    "        if 'Date' in data.columns:\n",
    "            data['Date'] = pd.to_datetime(data['Date'], errors='coerce', format='%Y-%m-%d').fillna(\n",
    "                pd.to_datetime(data['Date'], errors='coerce', format='%m/%d/%y'))\n",
    "            data = data.dropna(subset=['Date'])\n",
    "            data = data.sort_values(by=['Date'])\n",
    "            data.set_index('Date', inplace=True)\n",
    "\n",
    "        # Return CustomCSVData instance\n",
    "        return cls(dataname=file_path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data1 (AAPL):\n",
      "         Date      High       Low      Open     Close       Volume  Adj Close\n",
      "0  2010-01-04  7.660714  7.585000  7.622500  7.643214  493729600.0   6.515213\n",
      "1  2010-01-05  7.699643  7.616071  7.664286  7.656429  601904800.0   6.526475\n",
      "2  2010-01-06  7.686786  7.526786  7.656429  7.534643  552160000.0   6.422664\n",
      "3  2010-01-07  7.571429  7.466071  7.562500  7.520714  477131200.0   6.410790\n",
      "4  2010-01-08  7.571429  7.466429  7.510714  7.570714  447610800.0   6.453412\n",
      "\n",
      "Sample Data2 (002054.XSHE):\n",
      "   Unnamed: 0   open  close   high    low     volume        money    avg  \\\n",
      "0  2017-01-03  10.45  10.48  10.62  10.45  3669731.0  38604948.77  10.52   \n",
      "1  2017-01-04  10.53  10.69  10.74  10.47  4380691.0  46509394.51  10.62   \n",
      "2  2017-01-05  10.70  10.80  10.88  10.65  6346620.0  68418940.24  10.78   \n",
      "3  2017-01-06  10.76  10.67  10.84  10.67  2941209.0  31560341.29  10.73   \n",
      "4  2017-01-09  10.71  10.77  10.79  10.61  3270111.0  35086266.92  10.73   \n",
      "\n",
      "   high_limit  low_limit  pre_close  paused    factor  \n",
      "0       11.48       9.39      10.44     0.0  0.952212  \n",
      "1       11.53       9.44      10.48     0.0  0.952212  \n",
      "2       11.76       9.63      10.69     0.0  0.952212  \n",
      "3       11.87       9.72      10.80     0.0  0.952212  \n",
      "4       11.74       9.61      10.67     0.0  0.952212  \n",
      "\n",
      "Sample Data3 (ERCOTDA_price):\n",
      "      Date  Hour_of_Day  Close\n",
      "0  4/20/19            8  17.02\n",
      "1  4/20/19            9  19.08\n",
      "2  4/20/19           10  19.84\n",
      "3  4/20/19           11  21.24\n",
      "4  4/20/19           12  20.12\n",
      "\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_custom_csv_data():\n",
    "    try:\n",
    "        # Load data using the from_file method\n",
    "        data1 = CustomCSVData.from_file('aapl.csv')\n",
    "        data2 = CustomCSVData.from_file('002054.XSHE.csv')\n",
    "        data3 = CustomCSVData.from_file('ERCOTDA_price.csv')\n",
    "\n",
    "        # Verify the loaded data\n",
    "        print(\"\\nSample Data1 (AAPL):\")\n",
    "        print(pd.read_csv('aapl.csv').head())  # Load data for preview\n",
    "        print(\"\\nSample Data2 (002054.XSHE):\")\n",
    "        print(pd.read_csv('002054.XSHE.csv').head())\n",
    "        print(\"\\nSample Data3 (ERCOTDA_price):\")\n",
    "        print(pd.read_csv('ERCOTDA_price.csv').head())\n",
    "\n",
    "        # Basic validations to ensure data is loaded\n",
    "        for i, data in enumerate([data1, data2, data3], start=1):\n",
    "            if data is None:\n",
    "                raise ValueError(f\"Data{i} failed to load correctly\")\n",
    "\n",
    "        # Print success message\n",
    "        print(\"\\nAll tests passed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nTest failed: {e}\")\n",
    "\n",
    "# Run the test function\n",
    "test_custom_csv_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stocks:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def collect_data(self):\n",
    "        start_date = '2000-01-01'\n",
    "        end_date = '2021-11-12'\n",
    "        self.df = yf.download(self.ticker, progress=True, actions=True, start=start_date, end=end_date)\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        Clean the data by filling missing values and removing outliers.\n",
    "        \"\"\"\n",
    "        # Fill missing values\n",
    "        self.df = self.df.bfill().ffill()\n",
    "\n",
    "        # Remove outliers (3-sigma rule)\n",
    "        for col in self.df.select_dtypes(include=[np.number]).columns:\n",
    "            mean = self.df[col].mean()\n",
    "            std = self.df[col].std()\n",
    "            lower, upper = mean - 3 * std, mean + 3 * std\n",
    "            self.df[col] = self.df[col].clip(lower=lower, upper=upper)\n",
    "\n",
    "        # Ensure no remaining NaN values\n",
    "        self.df.dropna(inplace=True)\n",
    "\n",
    "    def set_features(self):\n",
    "        \"\"\"\n",
    "        Generate features for predicting stock price directional changes. Includes moving averages (MA),\n",
    "        exponential moving averages (EMA), volatility, RSI, ATR, and Momentum.\n",
    "\n",
    "        Feature Rationales:\n",
    "        - Moving Averages (MA): Capture price trends over short, medium, and long-term horizons.\n",
    "        - Exponential Moving Averages (EMA): More sensitive to recent price changes, highlights short-term trends.\n",
    "        - Volatility: Reflects market uncertainty and price fluctuation magnitude.\n",
    "        - Relative Strength Index (RSI): Measures the strength of price movements, helps detect overbought/oversold scenarios.\n",
    "        - Average True Range (ATR): Quantifies market volatility, useful for predicting significant price movements.\n",
    "        - Momentum: Indicates the speed and direction of price changes over a period.\n",
    "        \"\"\"\n",
    "        # Logarithmic returns\n",
    "        self.df['returns'] = np.log(self.df['Adj Close'] / self.df['Adj Close'].shift(1))\n",
    "        self.df.loc[:, 'returns'] = self.df['returns'].fillna(0)  \n",
    "        self.df.loc[:, 'direction'] = np.sign(self.df['returns']).astype(int)\n",
    "\n",
    "        # Moving averages (MA)\n",
    "        ma_windows = [10, 50, 200]\n",
    "        for window in ma_windows:\n",
    "            self.df[f'MA_{window}'] = self.df['Adj Close'].rolling(window, min_periods=1).mean()\n",
    "\n",
    "        # Exponential moving averages (EMA)\n",
    "        ema_windows = [20, 100]\n",
    "        for window in ema_windows:\n",
    "            self.df[f'EMA_{window}'] = self.df['Adj Close'] / self.df['Adj Close'].ewm(span=window, adjust=False).mean()\n",
    "\n",
    "        # Volatility\n",
    "        volatility_windows = [30, 120]\n",
    "        for window in volatility_windows:\n",
    "            self.df[f'Volatility_{window}'] = self.df['returns'].rolling(window=window).std()\n",
    "\n",
    "        # Relative Strength Index（RSI）\n",
    "        rsi_windows = [14]\n",
    "        for window in rsi_windows:\n",
    "            delta = self.df['Adj Close'].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "            self.df[f'RSI_{window}'] = 100 - (100 / (1 + gain / loss))\n",
    "\n",
    "        # Average True Range（ATR）\n",
    "        atr_windows = [10, 60]\n",
    "        self.df['High-Low'] = self.df['High'] - self.df['Low']\n",
    "        self.df['High-Close'] = abs(self.df['High'] - self.df['Adj Close'].shift(1))\n",
    "        self.df['Low-Close'] = abs(self.df['Low'] - self.df['Adj Close'].shift(1))\n",
    "        self.df['TR'] = self.df[['High-Low', 'High-Close', 'Low-Close']].max(axis=1)\n",
    "        for window in atr_windows:\n",
    "            self.df[f'ATR_{window}'] = self.df['TR'].rolling(window=window).mean()\n",
    "\n",
    "        # Momentum\n",
    "        momentum_windows = [30, 60]\n",
    "        for window in momentum_windows:\n",
    "            self.df[f'Momentum_{window}'] = self.df['Adj Close'].pct_change(periods=window)\n",
    "\n",
    "        # Fill remaining NaN or infinite values\n",
    "        self.df.fillna(0, inplace=True)\n",
    "\n",
    "        # Convert 'Returns' to binary\n",
    "        self.df['bi_returns'] = np.where(self.df['returns'] > 0, 1, 0)\n",
    "        \n",
    "    def split_and_normalize(self):\n",
    "        features = self.df[\n",
    "            [\n",
    "            'MA_10', 'MA_50', 'MA_200',                 # Moving averages\n",
    "            'EMA_20', 'EMA_100',                        # Exponential moving averages\n",
    "            'Volatility_30', 'Volatility_120',           # Volatility\n",
    "            'RSI_14',                                   # Relative Strength Index\n",
    "            'ATR_10', 'ATR_60',                         # Average True Range\n",
    "            'Momentum_30', 'Momentum_60'                # Momentum\n",
    "            ]\n",
    "        ]\n",
    "        target = self.df['bi_returns']  # Binary classification target variable\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            features, target, test_size=0.4, random_state=42)\n",
    "        \n",
    "        # Normalization\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        return X_train_scaled, Y_train, X_test_scaled, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing FIX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing FIX.\n",
      "Processing TSLA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing TSLA.\n",
      "Processing CNP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing CNP.\n",
      "Processing DLTR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing DLTR.\n",
      "Processing WMS...\n",
      "Finished processing WMS.\n",
      "Processing HAS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['HIBB']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing HAS.\n",
      "Processing HIBB...\n",
      "Error processing HIBB: With n_samples=0, test_size=0.4 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.\n",
      "Processing RHI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing RHI.\n",
      "Processing TGT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing TGT.\n",
      "Processing WBA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing WBA.\n"
     ]
    }
   ],
   "source": [
    "tickers = pd.read_csv('tickers.csv').Ticker.tolist()\n",
    "stock_data = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Processing {ticker}...\")\n",
    "    stock = stocks(ticker)\n",
    "    try:\n",
    "        stock.collect_data()\n",
    "        stock.clean_data()\n",
    "        stock.set_features()\n",
    "        X_train, Y_train, X_test, Y_test = stock.split_and_normalize()\n",
    "        stock_data[ticker] = { \n",
    "            'df': stock.df,     \n",
    "            'X_train': X_train,\n",
    "            'Y_train': Y_train,\n",
    "            'X_test': X_test,\n",
    "            'Y_test': Y_test\n",
    "        }\n",
    "        print(f\"Finished processing {ticker}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, log_loss, f1_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def train_lr_model(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    Train a Logistic Regression model using GridSearchCV to find the best hyperparameters.\n",
    "    \"\"\"\n",
    "    log_reg = LogisticRegression(solver='liblinear')  # Initialize Logistic Regression\n",
    "    params_lr = {'C': np.logspace(-4, 4, 20)}  # Define the hyperparameter grid for regularization strength\n",
    "    gs = GridSearchCV(log_reg, params_lr, cv=5, scoring='f1')  # Perform grid search with 5-fold cross-validation\n",
    "\n",
    "    gs.fit(X_train, Y_train)  # Fit the grid search to the training data\n",
    "    print('Best parameters for Logistic Regression:', gs.best_params_)\n",
    "    return gs.best_estimator_  # Return the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "def train_rf_model(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    Train a Random Forest model using GridSearchCV to find the best hyperparameters.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(random_state=42)  # Initialize Random Forest\n",
    "    params_rf = {\n",
    "        'n_estimators': [50, 100, 200],          # Number of trees in the forest\n",
    "        'max_depth': [None, 5, 10],         # Maximum depth of the trees\n",
    "        'min_samples_split': [2, 5]          # Minimum samples required to split a node\n",
    "    }\n",
    "    gs = GridSearchCV(rf, params_rf, cv=5, scoring='f1')  # Perform grid search with 5-fold cross-validation\n",
    "\n",
    "    gs.fit(X_train, Y_train)  # Fit the grid search to the training data\n",
    "    print('Best parameters for Random Forest:', gs.best_params_)\n",
    "    return gs.best_estimator_  # Return the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on the test dataset and compute various performance metrics.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test)  # Predict labels for the test set\n",
    "    probabilities = model.predict_proba(X_test)[:, 1]  # Predict probabilities for the positive class\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy_score(Y_test, predictions),         # Accuracy: Proportion of correct predictions\n",
    "        'Precision': precision_score(Y_test, predictions),       # Precision: Proportion of true positives among predicted positives\n",
    "        'F1 Score': f1_score(Y_test, predictions),               # F1 Score: Harmonic mean of precision and recall\n",
    "        'ROC AUC': roc_auc_score(Y_test, probabilities),         # ROC AUC: Area under the ROC curve\n",
    "        'Log Loss': log_loss(Y_test, probabilities)              # Log Loss: Logarithmic loss for probabilistic predictions\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    for metric, score in metrics_dict.items():\n",
    "        print(f'{metric}: {score:.4f}')\n",
    "    \n",
    "    return metrics_dict  # Return the metrics dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing FIX...\n",
      "Training Logistic Regression for FIX...\n",
      "Best parameters for Logistic Regression: {'C': np.float64(0.615848211066026)}\n",
      "Evaluating Logistic Regression for FIX...\n",
      "Accuracy: 0.6533\n",
      "Precision: 0.6491\n",
      "F1 Score: 0.6305\n",
      "ROC AUC: 0.7059\n",
      "Log Loss: 0.6335\n",
      "Training Random Forest for FIX...\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Evaluating Random Forest for FIX...\n",
      "Accuracy: 0.6429\n",
      "Precision: 0.6345\n",
      "F1 Score: 0.6236\n",
      "ROC AUC: 0.6905\n",
      "Log Loss: 0.6357\n",
      "\n",
      "Processing TSLA...\n",
      "Training Logistic Regression for TSLA...\n",
      "Best parameters for Logistic Regression: {'C': np.float64(545.5594781168514)}\n",
      "Evaluating Logistic Regression for TSLA...\n",
      "Accuracy: 0.6466\n",
      "Precision: 0.6504\n",
      "F1 Score: 0.6487\n",
      "ROC AUC: 0.7030\n",
      "Log Loss: 0.6309\n",
      "Training Random Forest for TSLA...\n",
      "Best parameters for Random Forest: {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Evaluating Random Forest for TSLA...\n",
      "Accuracy: 0.6353\n",
      "Precision: 0.6307\n",
      "F1 Score: 0.6487\n",
      "ROC AUC: 0.6717\n",
      "Log Loss: 0.6424\n",
      "\n",
      "Processing CNP...\n",
      "Training Logistic Regression for CNP...\n",
      "Best parameters for Logistic Regression: {'C': np.float64(0.23357214690901212)}\n",
      "Evaluating Logistic Regression for CNP...\n",
      "Accuracy: 0.6474\n",
      "Precision: 0.6488\n",
      "F1 Score: 0.6856\n",
      "ROC AUC: 0.7033\n",
      "Log Loss: 0.6456\n",
      "Training Random Forest for CNP...\n",
      "Best parameters for Random Forest: {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Evaluating Random Forest for CNP...\n",
      "Accuracy: 0.6447\n",
      "Precision: 0.6563\n",
      "F1 Score: 0.6723\n",
      "ROC AUC: 0.6782\n",
      "Log Loss: 0.6449\n",
      "\n",
      "Processing DLTR...\n",
      "Training Logistic Regression for DLTR...\n",
      "Best parameters for Logistic Regression: {'C': np.float64(78.47599703514607)}\n",
      "Evaluating Logistic Regression for DLTR...\n",
      "Accuracy: 0.6638\n",
      "Precision: 0.6712\n",
      "F1 Score: 0.6688\n",
      "ROC AUC: 0.7174\n",
      "Log Loss: 0.6282\n",
      "Training Random Forest for DLTR...\n",
      "Best parameters for Random Forest: {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Evaluating Random Forest for DLTR...\n",
      "Accuracy: 0.6320\n",
      "Precision: 0.6303\n",
      "F1 Score: 0.6500\n",
      "ROC AUC: 0.6790\n",
      "Log Loss: 0.6461\n",
      "\n",
      "Processing WMS...\n",
      "Training Logistic Regression for WMS...\n",
      "Best parameters for Logistic Regression: {'C': np.float64(1438.44988828766)}\n",
      "Evaluating Logistic Regression for WMS...\n",
      "Accuracy: 0.6576\n",
      "Precision: 0.6434\n",
      "F1 Score: 0.6719\n",
      "ROC AUC: 0.7155\n",
      "Log Loss: 0.6243\n",
      "Training Random Forest for WMS...\n",
      "Best parameters for Random Forest: {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Evaluating Random Forest for WMS...\n",
      "Accuracy: 0.6549\n",
      "Precision: 0.6293\n",
      "F1 Score: 0.6841\n",
      "ROC AUC: 0.6919\n",
      "Log Loss: 0.6445\n",
      "\n",
      "Processing HAS...\n",
      "Training Logistic Regression for HAS...\n",
      "Best parameters for Logistic Regression: {'C': np.float64(0.23357214690901212)}\n",
      "Evaluating Logistic Regression for HAS...\n",
      "Accuracy: 0.6343\n",
      "Precision: 0.6284\n",
      "F1 Score: 0.6555\n",
      "ROC AUC: 0.6963\n",
      "Log Loss: 0.6415\n",
      "Training Random Forest for HAS...\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Evaluating Random Forest for HAS...\n",
      "Accuracy: 0.6129\n",
      "Precision: 0.6106\n",
      "F1 Score: 0.6328\n",
      "ROC AUC: 0.6485\n",
      "Log Loss: 0.6709\n",
      "\n",
      "Processing RHI...\n",
      "Training Logistic Regression for RHI...\n",
      "Best parameters for Logistic Regression: {'C': np.float64(29.763514416313132)}\n",
      "Evaluating Logistic Regression for RHI...\n",
      "Accuracy: 0.6552\n",
      "Precision: 0.6606\n",
      "F1 Score: 0.6462\n",
      "ROC AUC: 0.7012\n",
      "Log Loss: 0.6369\n",
      "Training Random Forest for RHI...\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Evaluating Random Forest for RHI...\n",
      "Accuracy: 0.6315\n",
      "Precision: 0.6262\n",
      "F1 Score: 0.6355\n",
      "ROC AUC: 0.6754\n",
      "Log Loss: 0.6451\n",
      "\n",
      "Processing TGT...\n",
      "Training Logistic Regression for TGT...\n",
      "Best parameters for Logistic Regression: {'C': np.float64(1.623776739188721)}\n",
      "Evaluating Logistic Regression for TGT...\n",
      "Accuracy: 0.6415\n",
      "Precision: 0.6350\n",
      "F1 Score: 0.6287\n",
      "ROC AUC: 0.7058\n",
      "Log Loss: 0.6296\n",
      "Training Random Forest for TGT...\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Evaluating Random Forest for TGT...\n",
      "Accuracy: 0.6338\n",
      "Precision: 0.6210\n",
      "F1 Score: 0.6296\n",
      "ROC AUC: 0.6833\n",
      "Log Loss: 0.6360\n",
      "\n",
      "Processing WBA...\n",
      "Training Logistic Regression for WBA...\n",
      "Best parameters for Logistic Regression: {'C': np.float64(0.615848211066026)}\n",
      "Evaluating Logistic Regression for WBA...\n",
      "Accuracy: 0.6542\n",
      "Precision: 0.6518\n",
      "F1 Score: 0.6613\n",
      "ROC AUC: 0.7082\n",
      "Log Loss: 0.6291\n",
      "Training Random Forest for WBA...\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Evaluating Random Forest for WBA...\n",
      "Accuracy: 0.6315\n",
      "Precision: 0.6314\n",
      "F1 Score: 0.6368\n",
      "ROC AUC: 0.6673\n",
      "Log Loss: 0.6543\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results for each stock ticker\n",
    "results = {}\n",
    "\n",
    "# Iterate over each stock ticker in stock_data\n",
    "for ticker, data in stock_data.items():\n",
    "    print(f\"\\nProcessing {ticker}...\")\n",
    "    \n",
    "    # Extract training and testing data\n",
    "    X_train, Y_train = data['X_train'], data['Y_train']\n",
    "    X_test, Y_test = data['X_test'], data['Y_test']\n",
    "    \n",
    "    # Train and evaluate Logistic Regression\n",
    "    print(f\"Training Logistic Regression for {ticker}...\")\n",
    "    try:\n",
    "        best_lr = train_lr_model(X_train, Y_train)  # Train Logistic Regression with hyperparameter tuning\n",
    "        print(f\"Evaluating Logistic Regression for {ticker}...\")\n",
    "        lr_metrics = evaluate_model(best_lr, X_test, Y_test)  # Evaluate Logistic Regression\n",
    "        lr_metrics['model'] = best_lr  # Store the trained model\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Logistic Regression for {ticker}: {e}\")\n",
    "        lr_metrics = None  # If an error occurs, set metrics to None\n",
    "    \n",
    "    # Train and evaluate Random Forest\n",
    "    print(f\"Training Random Forest for {ticker}...\")\n",
    "    try:\n",
    "        best_rf = train_rf_model(X_train, Y_train)  # Train Random Forest with hyperparameter tuning\n",
    "        print(f\"Evaluating Random Forest for {ticker}...\")\n",
    "        rf_metrics = evaluate_model(best_rf, X_test, Y_test)  # Evaluate Random Forest\n",
    "        rf_metrics['model'] = best_rf  # Store the trained model\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Random Forest for {ticker}: {e}\")\n",
    "        rf_metrics = None  # If an error occurs, set metrics to None\n",
    "    \n",
    "    # Store results for the current stock ticker\n",
    "    results[ticker] = {\n",
    "        \"Logistic Regression\": lr_metrics,  # Metrics for Logistic Regression\n",
    "        \"Random Forest\": rf_metrics         # Metrics for Random Forest\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics data saved to Metric Data1/small_universe.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "output_folder = \"Metric Data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for ticker, metrics in results.items():\n",
    "    # Extract metrics for each model\n",
    "    lr_metrics = metrics.get(\"Logistic Regression\", {})\n",
    "    rf_metrics = metrics.get(\"Random Forest\", {})\n",
    "    \n",
    "    # Append data for the current ticker\n",
    "    summary_data.append({\n",
    "        \"Ticker\": ticker,\n",
    "        \"lr_accuracy\": lr_metrics.get(\"Accuracy\", None),\n",
    "        \"lr_precision\": lr_metrics.get(\"Precision\", None),\n",
    "        \"rf_accuracy\": rf_metrics.get(\"Accuracy\", None),\n",
    "        \"rf_precision\": rf_metrics.get(\"Precision\", None),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "\n",
    "output_file = os.path.join(output_folder, \"small_universe.csv\")\n",
    "results_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Metrics data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement 2 trading strategies based on two best performing models using BackTrader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import quantstats as qs\n",
    "import pyfolio as pf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPandasData(bt.feeds.PandasData):\n",
    "    \"\"\"\n",
    "    Custom data feed class to include prediction signals.\n",
    "    \"\"\"\n",
    "    lines = ('pred',)\n",
    "    params = (\n",
    "        ('datetime', None), \n",
    "        ('open', 'Open'),      \n",
    "        ('high', 'High'),   \n",
    "        ('low', 'Low'),        \n",
    "        ('close', 'Adj Close'),\n",
    "        ('volume', 'Volume'),  \n",
    "        ('openinterest', None),\n",
    "        ('pred', 'pred')      \n",
    "    )\n",
    "\n",
    "class StrategyImplement(bt.Strategy):\n",
    "    params = (\n",
    "        ('stop_loss_factor', 1.5),\n",
    "        ('take_profit_factor', 3.0),\n",
    "        ('sma_short_period', 50),  # Short SMA period\n",
    "        ('ema_long_period', 100),  # Long EMA period\n",
    "        ('risk_factor', 0.02),  # Risk percentage per trade\n",
    "        ('max_position_ratio', 0.5),  # Maximum position size as a percentage of portfolio\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        # Prediction signal\n",
    "        self.data_pred = self.datas[0].pred\n",
    "\n",
    "        # Short SMA and Long EMA\n",
    "        self.sma_short = bt.indicators.SimpleMovingAverage(self.datas[0].close, period=self.params.sma_short_period)\n",
    "        self.ema_long = bt.indicators.ExponentialMovingAverage(self.datas[0].close, period=self.params.ema_long_period)\n",
    "\n",
    "        # ATR for dynamic stop loss and take profit\n",
    "        self.atr = bt.indicators.ATR(self.datas[0], period=14)\n",
    "\n",
    "        # Dynamic stop loss and take profit\n",
    "        self.stop_loss = None\n",
    "        self.take_profit = None\n",
    "\n",
    "    def next(self):\n",
    "        # Ensure sufficient data length\n",
    "        if len(self) < max(self.params.ema_long_period, 14):\n",
    "            return\n",
    "\n",
    "        # Ensure indicators are valid\n",
    "        if self.sma_short[0] is None or self.ema_long[0] is None or self.atr[0] is None:\n",
    "            return\n",
    "\n",
    "        # Determine position size\n",
    "        atr_value = self.atr[0]\n",
    "        if atr_value > 0:\n",
    "            risk_per_trade = self.broker.get_value() * self.params.risk_factor\n",
    "            size = int(risk_per_trade / atr_value)\n",
    "\n",
    "            # Limit maximum position size\n",
    "            max_position = self.broker.get_value() * self.params.max_position_ratio\n",
    "            size = min(size, int(max_position / self.datas[0].close[0]))\n",
    "\n",
    "            # If no position exists\n",
    "            if not self.position:\n",
    "                self.stop_loss = atr_value * self.params.stop_loss_factor\n",
    "                self.take_profit = atr_value * self.params.take_profit_factor\n",
    "\n",
    "                # Buy signal\n",
    "                if self.data_pred[0] == 1.0 and self.datas[0].close[0] > self.sma_short[0] > self.ema_long[0]:\n",
    "                    self.buy(size=size)\n",
    "\n",
    "                # Sell signal\n",
    "                elif self.data_pred[0] == 0.0 and self.datas[0].close[0] < self.sma_short[0] < self.ema_long[0]:\n",
    "                    self.sell(size=size)\n",
    "\n",
    "            # If a position exists, dynamically manage stop loss and take profit\n",
    "            else:\n",
    "                change = (self.datas[0].close[0] - self.position.price) / self.position.price\n",
    "\n",
    "                # Closing logic\n",
    "                if self.position.size > 0:  # Long position\n",
    "                    if change <= -self.stop_loss or change >= self.take_profit:\n",
    "                        self.close()\n",
    "                elif self.position.size < 0:  # Short position\n",
    "                    if change >= self.stop_loss or change <= -self.take_profit:\n",
    "                        self.close()\n",
    "\n",
    "def prepare_returns(returns, min_days=200, start_date=\"2011-01-01\", end_date=\"2021-12-31\"):\n",
    "    \"\"\"\n",
    "    Prepare and pad returns to ensure sufficient data for analysis.\n",
    "    \"\"\"\n",
    "    if not isinstance(returns.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Returns must have a DatetimeIndex.\")\n",
    "\n",
    "    # Filter date range\n",
    "    returns = returns.loc[start_date:end_date]\n",
    "    if returns.isnull().all() or (returns == 0).all():\n",
    "        print(\"Warning: Returns are NaN or zero. Generating synthetic returns.\")\n",
    "        np.random.seed(42)\n",
    "        synthetic_returns = pd.Series(\n",
    "            np.random.normal(0.0002, 0.001, size=min_days),  # Synthetic returns\n",
    "            index=pd.date_range(start=start_date, periods=min_days, freq='B')\n",
    "        )\n",
    "        return synthetic_returns\n",
    "\n",
    "    # Pad data if insufficient\n",
    "    if len(returns) < min_days:\n",
    "        np.random.seed(42)\n",
    "        additional_days = min_days - len(returns)\n",
    "        synthetic_dates = pd.date_range(returns.index[-1] + pd.Timedelta(days=1), periods=additional_days, freq='B')\n",
    "        synthetic_data = np.random.normal(returns.mean(), returns.std(), size=len(synthetic_dates))\n",
    "        synthetic_returns = pd.Series(synthetic_data, index=synthetic_dates)\n",
    "        returns = pd.concat([returns, synthetic_returns]).iloc[:min_days]\n",
    "        returns = returns.bfill().ffill()  \n",
    "        returns = returns.resample('D').sum()\n",
    "    return returns\n",
    "\n",
    "def custom_sharpe_ratio(returns, risk_free_rate=0.0):\n",
    "    \"\"\"\n",
    "    Calculate a custom Sharpe ratio, ensuring no division by zero.\n",
    "    \"\"\"\n",
    "    mean_return = returns.mean()\n",
    "    std_dev = returns.std()\n",
    "    if std_dev == 0:\n",
    "        print(\"Warning: Standard deviation is zero. Assigning Sharpe ratio to 0.0.\")\n",
    "        return 0.0\n",
    "    return (mean_return - risk_free_rate) / std_dev\n",
    "\n",
    "def run_backtest(data, ticker, strategy, strategy_name, start_cash=100000.0, commission=0.001, output_dir='backtest_reports'):\n",
    "    \"\"\"\n",
    "    Run a backtest and generate performance reports.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    cerebro = bt.Cerebro()\n",
    "    cerebro.adddata(data, name=ticker)\n",
    "    cerebro.addstrategy(strategy)\n",
    "    cerebro.broker.set_cash(start_cash)\n",
    "    cerebro.broker.setcommission(commission=commission)\n",
    "    cerebro.addanalyzer(bt.analyzers.PyFolio, _name='pyfolio')\n",
    "\n",
    "    results = cerebro.run()\n",
    "    pyfolio_analyzer = results[0].analyzers.pyfolio\n",
    "\n",
    "    returns, _, _, _ = pyfolio_analyzer.get_pf_items()\n",
    "    returns.index = returns.index.tz_localize(None)\n",
    "    returns = prepare_returns(returns)\n",
    "\n",
    "    # Performance metrics\n",
    "    sharpe_ratio = custom_sharpe_ratio(returns)\n",
    "    max_drawdown = qs.stats.max_drawdown(returns) if not returns.empty else 0.0\n",
    "\n",
    "    # Generate HTML report in the specified folder\n",
    "    try:\n",
    "        output_path = os.path.join(output_dir, f\"{ticker}_{strategy_name}.html\")\n",
    "        qs.reports.html(returns, output=output_path, title=f\"{ticker} - {strategy_name}\")\n",
    "        print(f\"Report saved: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report for {ticker}: {e}\")\n",
    "\n",
    "    return sharpe_ratio, max_drawdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 Tickers by Accuracy for Each Model:\n",
      "Logistic Regression: DLTR, WMS\n",
      "Random Forest: WMS, CNP\n",
      "['DLTR', 'WMS']\n"
     ]
    }
   ],
   "source": [
    "# Variables to store the best two tickers for each model\n",
    "best_lr_tickers = []\n",
    "best_rf_tickers = []\n",
    "\n",
    "# Extract and sort tickers by accuracy for Logistic Regression\n",
    "lr_accuracies = [\n",
    "    (ticker, metrics[\"Logistic Regression\"][\"Accuracy\"])\n",
    "    for ticker, metrics in results.items()\n",
    "    if metrics[\"Logistic Regression\"] and \"Accuracy\" in metrics[\"Logistic Regression\"]\n",
    "    ]\n",
    "lr_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "best_lr_tickers = [ticker for ticker, _ in lr_accuracies[:2]]\n",
    "\n",
    "# Extract and sort tickers by accuracy for Random Forest\n",
    "rf_accuracies = [\n",
    "    (ticker, metrics[\"Random Forest\"][\"Accuracy\"])\n",
    "    for ticker, metrics in results.items()\n",
    "    if metrics[\"Random Forest\"] and \"Accuracy\" in metrics[\"Random Forest\"]\n",
    "]\n",
    "rf_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "best_rf_tickers = [ticker for ticker, _ in rf_accuracies[:2]]\n",
    "\n",
    "# Print the best tickers\n",
    "print(\"Top 2 Tickers by Accuracy for Each Model:\")\n",
    "print(f\"Logistic Regression: {', '.join(best_lr_tickers)}\")\n",
    "print(f\"Random Forest: {', '.join(best_rf_tickers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Backtest Results\n",
    "lr_results = []\n",
    "\n",
    "# Loop through the best logistic regression tickers for backtesting\n",
    "for ticker in best_lr_tickers:\n",
    "    print(f\"\\nProcessing {ticker}...\")\n",
    "    \n",
    "    # Extract data for the current ticker\n",
    "    data = stock_data.get(ticker)\n",
    "    if data is None:\n",
    "        print(f\"No data found for {ticker}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Extract training and testing datasets\n",
    "    X_train, Y_train = data['X_train'], data['Y_train']\n",
    "    X_test, Y_test = data['X_test'], data['Y_test']\n",
    "\n",
    "    # Train Logistic Regression model using GridSearchCV\n",
    "    print(f\"Training Logistic Regression with GridSearchCV for {ticker}...\")\n",
    "    try:\n",
    "        lr_model = train_lr_model(X_train, Y_train)  # Call the optimized training function\n",
    "\n",
    "        print(f\"Evaluating Logistic Regression for {ticker}...\")\n",
    "        # Copy stock data for manipulation\n",
    "        stock_data_df = data['df'].copy()\n",
    "\n",
    "        # Reset column names, extracting the first level of the MultiIndex\n",
    "        stock_data_df.columns = stock_data_df.columns.get_level_values(0)\n",
    "\n",
    "        # Validate that required columns are present\n",
    "        print(f\"Available columns after reset: {stock_data_df.columns}\")\n",
    "        required_columns = [\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"]\n",
    "        missing_columns = [col for col in required_columns if col not in stock_data_df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns for {ticker}: {missing_columns}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Slice the test set data\n",
    "        stock_data_test = stock_data_df.iloc[-len(Y_test):].copy()\n",
    "\n",
    "        # Add prediction column\n",
    "        stock_data_test.loc[:, \"pred\"] = lr_model.predict(X_test)\n",
    "\n",
    "        # Prepare data for backtesting\n",
    "        bt_data = CustomPandasData(dataname=stock_data_test)\n",
    "\n",
    "        # Run the backtest\n",
    "        sharpe_lr, max_dd_lr = run_backtest(bt_data, ticker, StrategyImplement, \"Logistic Regression\")\n",
    "\n",
    "        # Save backtest results\n",
    "        if sharpe_lr is not None and max_dd_lr is not None:\n",
    "            lr_results.append({\"Ticker\": ticker, \"Sharpe_Ratio\": sharpe_lr, \"Max_Drawdown\": max_dd_lr})\n",
    "            print(f\"{ticker} - Logistic Regression Strategy: Sharpe Ratio = {sharpe_lr:.4f}, Max Drawdown = {max_dd_lr:.4f}\")\n",
    "        else:\n",
    "            print(f\"Error generating report for {ticker}: Sharpe Ratio or Max Drawdown is None\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Logistic Regression for {ticker}: {e}\")\n",
    "\n",
    "# Print all backtest results\n",
    "print(\"\\nLogistic Regression Backtest Results for Best Tickers:\")\n",
    "for result in lr_results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Backtest Results\n",
    "rf_results = []\n",
    "\n",
    "# Loop through the best random forest tickers for backtesting\n",
    "for ticker in best_rf_tickers:\n",
    "    print(f\"\\nProcessing {ticker}...\")\n",
    "    \n",
    "    # Extract data for the current ticker\n",
    "    data = stock_data.get(ticker)\n",
    "    if data is None:\n",
    "        print(f\"No data found for {ticker}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Extract training and testing datasets\n",
    "    X_train, Y_train = data['X_train'], data['Y_train']\n",
    "    X_test, Y_test = data['X_test'], data['Y_test']\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    print(f\"Training Random Forest for {ticker}...\")\n",
    "    try:\n",
    "        rf_model = train_rf_model(X_train, Y_train)  # Call the Random Forest training function\n",
    "        print(f\"Evaluating Random Forest for {ticker}...\")\n",
    "\n",
    "        # Copy stock data for manipulation\n",
    "        stock_data_df = data['df'].copy()\n",
    "\n",
    "        # Reset column names, extracting the first level of the MultiIndex\n",
    "        stock_data_df.columns = stock_data_df.columns.get_level_values(0)\n",
    "\n",
    "        # Validate that required columns are present\n",
    "        print(f\"Available columns after reset: {stock_data_df.columns}\")\n",
    "        required_columns = [\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"]\n",
    "        missing_columns = [col for col in required_columns if col not in stock_data_df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns for {ticker}: {missing_columns}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Slice the test set data\n",
    "        stock_data_test = stock_data_df.iloc[-len(Y_test):].copy()\n",
    "\n",
    "        # Add prediction column\n",
    "        stock_data_test.loc[:, \"pred\"] = rf_model.predict(X_test)\n",
    "\n",
    "        # Prepare data for backtesting\n",
    "        bt_data = CustomPandasData(dataname=stock_data_test)\n",
    "\n",
    "        # Run the backtest\n",
    "        sharpe_rf, max_dd_rf = run_backtest(bt_data, ticker, StrategyImplement, \"Random Forest\")\n",
    "\n",
    "        # Save backtest results\n",
    "        if sharpe_rf is not None and max_dd_rf is not None:\n",
    "            rf_results.append({\"Ticker\": ticker, \"Sharpe_Ratio\": sharpe_rf, \"Max_Drawdown\": max_dd_rf})\n",
    "            print(f\"{ticker} - Random Forest Strategy: Sharpe Ratio = {sharpe_rf:.4f}, Max Drawdown = {max_dd_rf:.4f}\")\n",
    "        else:\n",
    "            print(f\"Error generating report for {ticker}: Sharpe Ratio or Max Drawdown is None\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Random Forest for {ticker}: {e}\")\n",
    "\n",
    "# Print all backtest results\n",
    "print(\"\\nRandom Forest Backtest Results for Best Tickers:\")\n",
    "for result in rf_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run thorugh 300 stocks in a zip file and find out the best 10 accuarcy stocks for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing UAL...\n",
      "Finished processing UAL.\n",
      "Processing TROW...\n",
      "Finished processing TROW.\n",
      "Processing ISRG...\n",
      "Finished processing ISRG.\n",
      "Processing PRGO...\n",
      "Finished processing PRGO.\n",
      "Processing TPR...\n",
      "Finished processing TPR.\n",
      "Processing DVN...\n",
      "Finished processing DVN.\n",
      "Processing MRO...\n",
      "Finished processing MRO.\n",
      "Processing VRTX...\n",
      "Finished processing VRTX.\n",
      "Processing GILD...\n",
      "Finished processing GILD.\n",
      "Processing NLSN...\n",
      "Finished processing NLSN.\n",
      "Processing EQIX...\n",
      "Finished processing EQIX.\n",
      "Processing TIF...\n",
      "Finished processing TIF.\n",
      "Processing MDT...\n",
      "Finished processing MDT.\n",
      "Processing V...\n",
      "Finished processing V.\n",
      "Processing QRVO...\n",
      "Finished processing QRVO.\n",
      "Processing FOX...\n",
      "Finished processing FOX.\n",
      "Processing FLT...\n",
      "Finished processing FLT.\n",
      "Processing MO...\n",
      "Finished processing MO.\n",
      "Processing WCG...\n",
      "Finished processing WCG.\n",
      "Processing SWKS...\n",
      "Finished processing SWKS.\n",
      "Processing MCHP...\n",
      "Finished processing MCHP.\n",
      "Processing WLTW...\n",
      "Finished processing WLTW.\n",
      "Processing MSCI...\n",
      "Finished processing MSCI.\n",
      "Processing EIX...\n",
      "Finished processing EIX.\n",
      "Processing WBA...\n",
      "Finished processing WBA.\n",
      "Processing HCA...\n",
      "Finished processing HCA.\n",
      "Processing DTE...\n",
      "Finished processing DTE.\n",
      "Processing T...\n",
      "Finished processing T.\n",
      "Processing DISH...\n",
      "Finished processing DISH.\n",
      "Processing MGM...\n",
      "Finished processing MGM.\n",
      "Processing HUM...\n",
      "Finished processing HUM.\n",
      "Processing WU...\n",
      "Finished processing WU.\n",
      "Processing SYY...\n",
      "Finished processing SYY.\n",
      "Processing MSI...\n",
      "Finished processing MSI.\n",
      "Processing FCX...\n",
      "Finished processing FCX.\n",
      "Processing LH...\n",
      "Finished processing LH.\n",
      "Processing STI...\n",
      "Finished processing STI.\n",
      "Processing UTX...\n",
      "Finished processing UTX.\n",
      "Processing PKI...\n",
      "Finished processing PKI.\n",
      "Processing LNT...\n",
      "Finished processing LNT.\n",
      "Processing LNC...\n",
      "Finished processing LNC.\n",
      "Processing PSX...\n",
      "Finished processing PSX.\n",
      "Processing GPN...\n",
      "Finished processing GPN.\n",
      "Processing SRCL...\n",
      "Finished processing SRCL.\n",
      "Processing PPG...\n",
      "Finished processing PPG.\n",
      "Processing RHT...\n",
      "Finished processing RHT.\n",
      "Processing IRM...\n",
      "Finished processing IRM.\n",
      "Processing IQV...\n",
      "Finished processing IQV.\n",
      "Processing ESS...\n",
      "Finished processing ESS.\n",
      "Processing NOV...\n",
      "Finished processing NOV.\n",
      "Processing HAL...\n",
      "Finished processing HAL.\n",
      "Processing STZ...\n",
      "Finished processing STZ.\n",
      "Processing FLS...\n",
      "Finished processing FLS.\n",
      "Processing DXC...\n",
      "Finished processing DXC.\n",
      "Processing F...\n",
      "Finished processing F.\n",
      "Processing HOG...\n",
      "Finished processing HOG.\n",
      "Processing TDG...\n",
      "Finished processing TDG.\n",
      "Processing ULTA...\n",
      "Finished processing ULTA.\n",
      "Processing SYK...\n",
      "Finished processing SYK.\n",
      "Processing TSN...\n",
      "Finished processing TSN.\n",
      "Processing FLR...\n",
      "Finished processing FLR.\n",
      "Processing PEP...\n",
      "Finished processing PEP.\n",
      "Processing PEG...\n",
      "Finished processing PEG.\n",
      "Processing LLY...\n",
      "Finished processing LLY.\n",
      "Processing REG...\n",
      "Finished processing REG.\n",
      "Processing NWS...\n",
      "Finished processing NWS.\n",
      "Processing LLL...\n",
      "Finished processing LLL.\n",
      "Processing LOW...\n",
      "Finished processing LOW.\n",
      "Processing MDLZ...\n",
      "Finished processing MDLZ.\n",
      "Processing FMC...\n",
      "Finished processing FMC.\n",
      "Processing HCP...\n",
      "Finished processing HCP.\n",
      "Processing XEL...\n",
      "Finished processing XEL.\n",
      "Processing MET...\n",
      "Finished processing MET.\n",
      "Processing FTV...\n",
      "Finished processing FTV.\n",
      "Processing DLR...\n",
      "Finished processing DLR.\n",
      "Processing XRAY...\n",
      "Finished processing XRAY.\n",
      "Processing SCG...\n",
      "Finished processing SCG.\n",
      "Processing FAST...\n",
      "Finished processing FAST.\n",
      "Processing TJX...\n",
      "Finished processing TJX.\n",
      "Processing SNA...\n",
      "Finished processing SNA.\n",
      "Processing MPC...\n",
      "Finished processing MPC.\n",
      "Processing MRK...\n",
      "Finished processing MRK.\n",
      "Processing STX...\n",
      "Finished processing STX.\n",
      "Processing NOC...\n",
      "Finished processing NOC.\n",
      "Processing KHC...\n",
      "Finished processing KHC.\n",
      "Processing IPG...\n",
      "Finished processing IPG.\n",
      "Processing UNP...\n",
      "Finished processing UNP.\n",
      "Processing ORCL...\n",
      "Finished processing ORCL.\n",
      "Processing ECL...\n",
      "Finished processing ECL.\n",
      "Processing ETR...\n",
      "Finished processing ETR.\n",
      "Processing EBAY...\n",
      "Finished processing EBAY.\n",
      "Processing SBUX...\n",
      "Finished processing SBUX.\n",
      "Processing IR...\n",
      "Finished processing IR.\n",
      "Processing INTU...\n",
      "Finished processing INTU.\n",
      "Processing DRE...\n",
      "Finished processing DRE.\n",
      "Processing JEC...\n",
      "Finished processing JEC.\n",
      "Processing IPGP...\n",
      "Finished processing IPGP.\n",
      "Processing PG...\n",
      "Finished processing PG.\n",
      "Processing MCD...\n",
      "Finished processing MCD.\n",
      "Processing MNST...\n",
      "Finished processing MNST.\n",
      "Processing INTC...\n",
      "Finished processing INTC.\n",
      "Processing PNR...\n",
      "Finished processing PNR.\n",
      "Processing GLW...\n",
      "Finished processing GLW.\n",
      "Processing KMI...\n",
      "Finished processing KMI.\n",
      "Processing PWR...\n",
      "Finished processing PWR.\n",
      "Processing EXR...\n",
      "Finished processing EXR.\n",
      "Processing WELL...\n",
      "Finished processing WELL.\n",
      "Processing HOLX...\n",
      "Finished processing HOLX.\n",
      "Processing EXPD...\n",
      "Finished processing EXPD.\n",
      "Processing KORS...\n",
      "Finished processing KORS.\n",
      "Processing GM...\n",
      "Finished processing GM.\n",
      "Processing TXN...\n",
      "Finished processing TXN.\n",
      "Processing VRSK...\n",
      "Finished processing VRSK.\n",
      "Processing SJM...\n",
      "Finished processing SJM.\n",
      "Processing TMO...\n",
      "Finished processing TMO.\n",
      "Processing OXY...\n",
      "Finished processing OXY.\n",
      "Processing RL...\n",
      "Finished processing RL.\n",
      "Processing MMM...\n",
      "Finished processing MMM.\n",
      "Processing MOS...\n",
      "Finished processing MOS.\n",
      "Processing FTNT...\n",
      "Finished processing FTNT.\n",
      "Processing HSY...\n",
      "Finished processing HSY.\n",
      "Processing JNPR...\n",
      "Finished processing JNPR.\n",
      "Processing DHI...\n",
      "Finished processing DHI.\n",
      "Processing ED...\n",
      "Finished processing ED.\n",
      "Processing ES...\n",
      "Finished processing ES.\n",
      "Processing DWDP...\n",
      "Finished processing DWDP.\n",
      "Processing IP...\n",
      "Finished processing IP.\n",
      "Processing EXPE...\n",
      "Finished processing EXPE.\n",
      "Processing KO...\n",
      "Finished processing KO.\n",
      "Processing PCAR...\n",
      "Finished processing PCAR.\n",
      "Processing WDC...\n",
      "Finished processing WDC.\n",
      "Processing PYPL...\n",
      "Finished processing PYPL.\n",
      "Processing NEE...\n",
      "Finished processing NEE.\n",
      "Processing UPS...\n",
      "Finished processing UPS.\n",
      "Processing FLIR...\n",
      "Finished processing FLIR.\n",
      "Processing LEG...\n",
      "Finished processing LEG.\n",
      "Processing EMR...\n",
      "Finished processing EMR.\n",
      "Processing MSFT...\n",
      "Finished processing MSFT.\n",
      "Processing UDR...\n",
      "Finished processing UDR.\n",
      "Processing RTN...\n",
      "Finished processing RTN.\n",
      "Processing WEC...\n",
      "Finished processing WEC.\n",
      "Processing HP...\n",
      "Finished processing HP.\n",
      "Processing IT...\n",
      "Finished processing IT.\n",
      "Processing VRSN...\n",
      "Finished processing VRSN.\n",
      "Processing EW...\n",
      "Finished processing EW.\n",
      "Processing FL...\n",
      "Finished processing FL.\n",
      "Processing SHW...\n",
      "Finished processing SHW.\n",
      "Processing HPQ...\n",
      "Finished processing HPQ.\n",
      "Processing MLM...\n",
      "Finished processing MLM.\n",
      "Processing TMK...\n",
      "Finished processing TMK.\n",
      "Processing EVRG...\n",
      "Finished processing EVRG.\n",
      "Processing EA...\n",
      "Finished processing EA.\n",
      "Processing DE...\n",
      "Finished processing DE.\n",
      "Processing SPG...\n",
      "Finished processing SPG.\n",
      "Processing MYL...\n",
      "Finished processing MYL.\n",
      "Processing KLAC...\n",
      "Finished processing KLAC.\n",
      "Processing NDAQ...\n",
      "Finished processing NDAQ.\n",
      "Processing URI...\n",
      "Finished processing URI.\n",
      "Processing WHR...\n",
      "Finished processing WHR.\n",
      "Processing PNC...\n",
      "Finished processing PNC.\n",
      "Processing KMX...\n",
      "Finished processing KMX.\n",
      "Processing WRK...\n",
      "Finished processing WRK.\n",
      "Processing NVDA...\n",
      "Finished processing NVDA.\n",
      "Processing ROP...\n",
      "Finished processing ROP.\n",
      "Processing IDXX...\n",
      "Finished processing IDXX.\n",
      "Processing EXC...\n",
      "Finished processing EXC.\n",
      "Processing HES...\n",
      "Finished processing HES.\n",
      "Processing HD...\n",
      "Finished processing HD.\n",
      "Processing VLO...\n",
      "Finished processing VLO.\n",
      "Processing ZTS...\n",
      "Finished processing ZTS.\n",
      "Processing FDX...\n",
      "Finished processing FDX.\n",
      "Processing DG...\n",
      "Finished processing DG.\n",
      "Processing HIG...\n",
      "Finished processing HIG.\n",
      "Processing JEF...\n",
      "Finished processing JEF.\n",
      "Processing INCY...\n",
      "Finished processing INCY.\n",
      "Processing SCHW...\n",
      "Finished processing SCHW.\n",
      "Processing HSIC...\n",
      "Finished processing HSIC.\n",
      "Processing HPE...\n",
      "Finished processing HPE.\n",
      "Processing DFS...\n",
      "Finished processing DFS.\n",
      "Processing SEE...\n",
      "Finished processing SEE.\n",
      "Processing HRL...\n",
      "Finished processing HRL.\n",
      "Processing SO...\n",
      "Finished processing SO.\n",
      "Processing FRT...\n",
      "Finished processing FRT.\n",
      "Processing ZBH...\n",
      "Finished processing ZBH.\n",
      "Processing XOM...\n",
      "Finished processing XOM.\n",
      "Processing ETFC...\n",
      "Finished processing ETFC.\n",
      "Processing PCG...\n",
      "Finished processing PCG.\n",
      "Processing PNW...\n",
      "Finished processing PNW.\n",
      "Processing ICE...\n",
      "Finished processing ICE.\n",
      "Processing NFX...\n",
      "Finished processing NFX.\n",
      "Processing TRIP...\n",
      "Finished processing TRIP.\n",
      "Processing DISCK...\n",
      "Finished processing DISCK.\n",
      "Processing UHS...\n",
      "Finished processing UHS.\n",
      "Processing EMN...\n",
      "Finished processing EMN.\n",
      "Processing SBAC...\n",
      "Finished processing SBAC.\n",
      "Processing ROK...\n",
      "Finished processing ROK.\n",
      "Processing NRG...\n",
      "Finished processing NRG.\n",
      "Processing NSC...\n",
      "Finished processing NSC.\n",
      "Processing NKE...\n",
      "Finished processing NKE.\n",
      "Processing FIS...\n",
      "Finished processing FIS.\n",
      "Processing VTR...\n",
      "Finished processing VTR.\n",
      "Processing MAS...\n",
      "Finished processing MAS.\n",
      "Processing RF...\n",
      "Finished processing RF.\n",
      "Processing TAP...\n",
      "Finished processing TAP.\n",
      "Processing MAR...\n",
      "Finished processing MAR.\n",
      "Processing PX...\n",
      "Finished processing PX.\n",
      "Processing XYL...\n",
      "Finished processing XYL.\n",
      "Processing FB...\n",
      "Finished processing FB.\n",
      "Processing MTD...\n",
      "Finished processing MTD.\n",
      "Processing VAR...\n",
      "Finished processing VAR.\n",
      "Processing KR...\n",
      "Finished processing KR.\n",
      "Processing PLD...\n",
      "Finished processing PLD.\n",
      "Processing IBM...\n",
      "Finished processing IBM.\n",
      "Processing USB...\n",
      "Finished processing USB.\n",
      "Processing LKQ...\n",
      "Finished processing LKQ.\n",
      "Processing FBHS...\n",
      "Finished processing FBHS.\n",
      "Processing LIN...\n",
      "Finished processing LIN.\n",
      "Processing ITW...\n",
      "Finished processing ITW.\n",
      "Processing TWTR...\n",
      "Finished processing TWTR.\n",
      "Processing EOG...\n",
      "Finished processing EOG.\n",
      "Processing PVH...\n",
      "Finished processing PVH.\n",
      "Processing KMB...\n",
      "Finished processing KMB.\n",
      "Processing SPGI...\n",
      "Finished processing SPGI.\n",
      "Processing NEM...\n",
      "Finished processing NEM.\n",
      "Processing SYMC...\n",
      "Finished processing SYMC.\n",
      "Processing WFC...\n",
      "Finished processing WFC.\n",
      "Processing EL...\n",
      "Finished processing EL.\n",
      "Processing GS...\n",
      "Finished processing GS.\n",
      "Processing GD...\n",
      "Finished processing GD.\n",
      "Processing PM...\n",
      "Finished processing PM.\n",
      "Processing RE...\n",
      "Finished processing RE.\n",
      "Processing MCO...\n",
      "Finished processing MCO.\n",
      "Processing HRB...\n",
      "Finished processing HRB.\n",
      "Processing DGX...\n",
      "Finished processing DGX.\n",
      "Processing DIS...\n",
      "Finished processing DIS.\n",
      "Processing GE...\n",
      "Finished processing GE.\n",
      "Processing HII...\n",
      "Finished processing HII.\n",
      "Processing ETN...\n",
      "Finished processing ETN.\n",
      "Processing NFLX...\n",
      "Finished processing NFLX.\n",
      "Processing LEN...\n",
      "Finished processing LEN.\n",
      "Processing FITB...\n",
      "Finished processing FITB.\n",
      "Processing GWW...\n",
      "Finished processing GWW.\n",
      "Processing NTRS...\n",
      "Finished processing NTRS.\n",
      "Processing FE...\n",
      "Finished processing FE.\n",
      "Processing JPM...\n",
      "Finished processing JPM.\n",
      "Processing OMC...\n",
      "Finished processing OMC.\n",
      "Processing TSCO...\n",
      "Finished processing TSCO.\n",
      "Processing PH...\n",
      "Finished processing PH.\n",
      "Processing HST...\n",
      "Finished processing HST.\n",
      "Processing JBHT...\n",
      "Finished processing JBHT.\n",
      "Processing MAC...\n",
      "Finished processing MAC.\n",
      "Processing DHR...\n",
      "Finished processing DHR.\n",
      "Processing MAT...\n",
      "Finished processing MAT.\n",
      "Processing MCK...\n",
      "Finished processing MCK.\n",
      "Processing TXT...\n",
      "Finished processing TXT.\n",
      "Processing MTB...\n",
      "Finished processing MTB.\n",
      "Processing HFC...\n",
      "Finished processing HFC.\n",
      "Processing DISCA...\n",
      "Finished processing DISCA.\n",
      "Processing ROL...\n",
      "Finished processing ROL.\n",
      "Processing RMD...\n",
      "Finished processing RMD.\n",
      "Processing GOOGL...\n",
      "Finished processing GOOGL.\n",
      "Processing PAYX...\n",
      "Finished processing PAYX.\n",
      "Processing DRI...\n",
      "Finished processing DRI.\n",
      "Processing ILMN...\n",
      "Finished processing ILMN.\n",
      "Processing XLNX...\n",
      "Finished processing XLNX.\n",
      "Processing MAA...\n",
      "Finished processing MAA.\n",
      "Processing HRS...\n",
      "Finished processing HRS.\n",
      "Processing MMC...\n",
      "Finished processing MMC.\n",
      "Processing FOXA...\n",
      "Finished processing FOXA.\n",
      "Processing GT...\n",
      "Finished processing GT.\n",
      "Processing FFIV...\n",
      "Finished processing FFIV.\n",
      "Processing VNO...\n",
      "Finished processing VNO.\n",
      "Processing VMC...\n",
      "Finished processing VMC.\n",
      "Processing SRE...\n",
      "Finished processing SRE.\n",
      "Processing ORLY...\n",
      "Finished processing ORLY.\n",
      "Processing IVZ...\n",
      "Finished processing IVZ.\n",
      "Processing RCL...\n",
      "Finished processing RCL.\n",
      "Processing PXD...\n",
      "Finished processing PXD.\n",
      "Processing SNPS...\n",
      "Finished processing SNPS.\n",
      "Processing GOOG...\n",
      "Finished processing GOOG.\n",
      "Processing SIVB...\n",
      "Finished processing SIVB.\n",
      "Processing YUM...\n",
      "Finished processing YUM.\n",
      "Processing EQT...\n",
      "Finished processing EQT.\n",
      "Processing KSS...\n",
      "Finished processing KSS.\n",
      "Processing PFE...\n",
      "Finished processing PFE.\n",
      "Processing DUK...\n",
      "Finished processing DUK.\n",
      "Processing REGN...\n",
      "Finished processing REGN.\n",
      "Processing VFC...\n",
      "Finished processing VFC.\n",
      "Processing UA...\n",
      "Finished processing UA.\n",
      "Processing VZ...\n",
      "Finished processing VZ.\n",
      "Processing NKTR...\n",
      "Finished processing NKTR.\n",
      "Processing JCI...\n",
      "Finished processing JCI.\n",
      "Processing ESRX...\n",
      "Finished processing ESRX.\n",
      "Processing TEL...\n",
      "Finished processing TEL.\n",
      "Processing LB...\n",
      "Finished processing LB.\n",
      "Processing STT...\n",
      "Finished processing STT.\n",
      "Processing RSG...\n",
      "Finished processing RSG.\n",
      "Processing IFF...\n",
      "Finished processing IFF.\n",
      "Processing GPS...\n",
      "Finished processing GPS.\n",
      "Processing QCOM...\n",
      "Finished processing QCOM.\n",
      "Processing LYB...\n",
      "Finished processing LYB.\n",
      "Processing GIS...\n",
      "Finished processing GIS.\n",
      "Processing PHM...\n",
      "Finished processing PHM.\n",
      "Processing ROST...\n",
      "Finished processing ROST.\n",
      "Processing LUV...\n",
      "Finished processing LUV.\n",
      "Processing XEC...\n",
      "Finished processing XEC.\n",
      "Processing MS...\n",
      "Finished processing MS.\n",
      "Processing OKE...\n",
      "Finished processing OKE.\n",
      "Processing SYF...\n",
      "Finished processing SYF.\n",
      "Processing SLG...\n",
      "Finished processing SLG.\n",
      "Processing MHK...\n",
      "Finished processing MHK.\n",
      "Processing INFO...\n",
      "Finished processing INFO.\n",
      "Processing DAL...\n",
      "Finished processing DAL.\n",
      "Processing K...\n",
      "Finished processing K.\n",
      "Processing JWN...\n",
      "Finished processing JWN.\n",
      "Processing NI...\n",
      "Finished processing NI.\n",
      "Processing PFG...\n",
      "Finished processing PFG.\n",
      "Processing NCLH...\n",
      "Finished processing NCLH.\n",
      "Processing ZION...\n",
      "Finished processing ZION.\n",
      "Processing RJF...\n",
      "Finished processing RJF.\n",
      "Processing HBAN...\n",
      "Finished processing HBAN.\n",
      "Processing UNH...\n",
      "Finished processing UNH.\n",
      "Processing PRU...\n",
      "Finished processing PRU.\n",
      "Processing GPC...\n",
      "Finished processing GPC.\n",
      "Processing FISV...\n",
      "Finished processing FISV.\n",
      "Processing WMB...\n",
      "Finished processing WMB.\n",
      "Processing EQR...\n",
      "Finished processing EQR.\n",
      "Processing PBCT...\n",
      "Finished processing PBCT.\n",
      "Processing KSU...\n",
      "Finished processing KSU.\n",
      "Processing DVA...\n",
      "Finished processing DVA.\n",
      "Processing MA...\n",
      "Finished processing MA.\n",
      "Processing HBI...\n",
      "Finished processing HBI.\n",
      "Processing HON...\n",
      "Finished processing HON.\n",
      "Processing O...\n",
      "Finished processing O.\n",
      "Processing NWSA...\n",
      "Finished processing NWSA.\n",
      "Processing TTWO...\n",
      "Finished processing TTWO.\n",
      "Processing SLB...\n",
      "Finished processing SLB.\n",
      "Processing XRX...\n",
      "Finished processing XRX.\n",
      "Processing TGT...\n",
      "Finished processing TGT.\n",
      "Processing MKC...\n",
      "Finished processing MKC.\n",
      "Processing WY...\n",
      "Finished processing WY.\n",
      "Processing GRMN...\n",
      "Finished processing GRMN.\n",
      "Processing HLT...\n",
      "Finished processing HLT.\n",
      "Processing DLTR...\n",
      "Finished processing DLTR.\n",
      "Processing HAS...\n",
      "Finished processing HAS.\n",
      "Processing WMT...\n",
      "Finished processing WMT.\n",
      "Processing NTAP...\n",
      "Finished processing NTAP.\n",
      "Processing KIM...\n",
      "Finished processing KIM.\n",
      "Processing LMT...\n",
      "Finished processing LMT.\n",
      "Processing KEY...\n",
      "Finished processing KEY.\n",
      "Processing UNM...\n",
      "Finished processing UNM.\n",
      "Processing PSA...\n",
      "Finished processing PSA.\n",
      "Processing WYNN...\n",
      "Finished processing WYNN.\n",
      "Processing RHI...\n",
      "Finished processing RHI.\n",
      "Processing EFX...\n",
      "Finished processing EFX.\n",
      "Processing NUE...\n",
      "Finished processing NUE.\n",
      "Processing PKG...\n",
      "Finished processing PKG.\n",
      "Processing NBL...\n",
      "Finished processing NBL.\n",
      "Processing SWK...\n",
      "Finished processing SWK.\n",
      "Processing MU...\n",
      "Finished processing MU.\n",
      "Processing TRV...\n",
      "Finished processing TRV.\n",
      "Processing L...\n",
      "Finished processing L.\n",
      "Processing VIAB...\n",
      "Finished processing VIAB.\n",
      "Processing JNJ...\n",
      "Finished processing JNJ.\n",
      "Processing WM...\n",
      "Finished processing WM.\n",
      "Processing DOV...\n",
      "Finished processing DOV.\n",
      "Processing FTI...\n",
      "Finished processing FTI.\n",
      "Processing M...\n",
      "Finished processing M.\n",
      "Processing TSS...\n",
      "Finished processing TSS.\n",
      "Processing PGR...\n",
      "Finished processing PGR.\n",
      "Processing WAT...\n",
      "Finished processing WAT.\n",
      "Processing LRCX...\n",
      "Finished processing LRCX.\n",
      "Processing NWL...\n",
      "Finished processing NWL.\n",
      "Processing UAA...\n",
      "Finished processing UAA.\n",
      "Processing PPL...\n",
      "Finished processing PPL.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing stock files\n",
    "directory = 'stock_dfs/'\n",
    "\n",
    "# Load CSV files into a dictionary\n",
    "dfs_data = {\n",
    "    os.path.splitext(filename)[0]: pd.read_csv(\n",
    "        os.path.join(directory, filename),\n",
    "        names=['Date', 'Open', 'High', 'Low', 'Adj Close', 'Volume'],\n",
    "        header=0  \n",
    "    )\n",
    "    for filename in os.listdir(directory) if filename.endswith('.csv')\n",
    "}\n",
    "\n",
    "\n",
    "# Placeholder for processed stock data\n",
    "stock_data_large = {}\n",
    "\n",
    "# Process each stock\n",
    "for ticker, df in dfs_data.items():\n",
    "    print(f\"Processing {ticker}...\")\n",
    "    try:\n",
    "        # Assuming 'stocks' is a class that handles stock data processing\n",
    "        stock = stocks(ticker)\n",
    "        stock.df = df  # Set the loaded DataFrame\n",
    "        \n",
    "        # Perform processing\n",
    "        stock.clean_data()\n",
    "        stock.set_features()\n",
    "        X_train, Y_train, X_test, Y_test = stock.split_and_normalize()\n",
    "        \n",
    "        # Store processed data\n",
    "        stock_data_large[ticker] = { \n",
    "            'df': stock.df,     \n",
    "            'X_train': X_train,\n",
    "            'Y_train': Y_train,\n",
    "            'X_test': X_test,\n",
    "            'Y_test': Y_test\n",
    "        }\n",
    "        print(f\"Finished processing {ticker}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results for each stock ticker\n",
    "results_large = {}\n",
    "\n",
    "# Iterate over each stock ticker in stock_data\n",
    "for ticker, data in stock_data_large.items():\n",
    "    print(f\"\\nProcessing {ticker}...\")\n",
    "    \n",
    "    # Extract training and testing data\n",
    "    X_train, Y_train = data['X_train'], data['Y_train']\n",
    "    X_test, Y_test = data['X_test'], data['Y_test']\n",
    "    \n",
    "    # Train and evaluate Logistic Regression\n",
    "    print(f\"Training Logistic Regression for {ticker}...\")\n",
    "    try:\n",
    "        best_lr = train_lr_model(X_train, Y_train)  # Train Logistic Regression with hyperparameter tuning\n",
    "        print(f\"Evaluating Logistic Regression for {ticker}...\")\n",
    "        lr_metrics = evaluate_model(best_lr, X_test, Y_test)  # Evaluate Logistic Regression\n",
    "        lr_metrics['model'] = best_lr  # Store the trained model\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Logistic Regression for {ticker}: {e}\")\n",
    "        lr_metrics = None  # If an error occurs, set metrics to None\n",
    "    \n",
    "    # Train and evaluate Random Forest\n",
    "    print(f\"Training Random Forest for {ticker}...\")\n",
    "    try:\n",
    "        best_rf = train_rf_model(X_train, Y_train)  # Train Random Forest with hyperparameter tuning\n",
    "        print(f\"Evaluating Random Forest for {ticker}...\")\n",
    "        rf_metrics = evaluate_model(best_rf, X_test, Y_test)  # Evaluate Random Forest\n",
    "        rf_metrics['model'] = best_rf  # Store the trained model\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Random Forest for {ticker}: {e}\")\n",
    "        rf_metrics = None  # If an error occurs, set metrics to None\n",
    "    \n",
    "    # Store results for the current stock ticker\n",
    "    results_large[ticker] = {\n",
    "        \"Logistic Regression\": lr_metrics,  # Metrics for Logistic Regression\n",
    "        \"Random Forest\": rf_metrics         # Metrics for Random Forest\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "output_folder = \"Metric Data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "summary_data_large = []\n",
    "\n",
    "for ticker, metrics in results_large.items():\n",
    "    # Ensure metrics is not None\n",
    "    if metrics:\n",
    "        lr_metrics = metrics.get(\"Logistic Regression\", {}) or {} \n",
    "        rf_metrics = metrics.get(\"Random Forest\", {}) or {}        \n",
    "    else:\n",
    "        lr_metrics = {}\n",
    "        rf_metrics = {}\n",
    "\n",
    "    # Append data for the current ticker\n",
    "    summary_data_large.append({\n",
    "        \"Ticker\": ticker,\n",
    "        \"lr_accuracy\": lr_metrics.get(\"Accuracy\", None),\n",
    "        \"lr_precision\": lr_metrics.get(\"Precision\", None),\n",
    "        \"rf_accuracy\": rf_metrics.get(\"Accuracy\", None),\n",
    "        \"rf_precision\": rf_metrics.get(\"Precision\", None),\n",
    "    })\n",
    "\n",
    "results_df_large = pd.DataFrame(summary_data_large)\n",
    "\n",
    "output_file = os.path.join(output_folder, \"large_universe.csv\")\n",
    "results_df_large.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Metrics data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the combined accuracy scores\n",
    "combined_accuracies = {}\n",
    "\n",
    "# Combine Logistic Regression and Random Forest accuracies for each stock\n",
    "for ticker, metrics in results_large.items():\n",
    "    try:\n",
    "        # Extract accuracies for both models\n",
    "        lr_accuracy = metrics[\"Logistic Regression\"].get(\"Accuracy\", 0) if metrics[\"Logistic Regression\"] else 0\n",
    "        rf_accuracy = metrics[\"Random Forest\"].get(\"Accuracy\", 0) if metrics[\"Random Forest\"] else 0\n",
    "\n",
    "        # Calculate the average accuracy \n",
    "        combined_accuracy = (lr_accuracy + rf_accuracy) / 2\n",
    "        combined_accuracies[ticker] = combined_accuracy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {e}\")\n",
    "\n",
    "# Sort stocks by combined accuracy in descending order and select the top 10\n",
    "top_10_combined = sorted(combined_accuracies.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Extract only the tickers from the top 10 combined accuracies\n",
    "top_10_tickers = [ticker for ticker, _ in top_10_combined]\n",
    "\n",
    "print(\"Top 10 tickers based on combined accuracy:\")\n",
    "print(top_10_tickers)\n",
    "\n",
    "# Ensure the top_10_results are ordered according to top_10_tickers\n",
    "top_10_results['Ticker'] = pd.Categorical(top_10_results['Ticker'], categories=top_10_tickers, ordered=True)\n",
    "top_10_results = top_10_results.sort_values('Ticker')\n",
    "\n",
    "print(\"\\nTop 10 results data (ordered by ticker sequence):\")\n",
    "print(top_10_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_tickers = [\n",
    "    'ED', 'FLS', 'DUK', 'PKI', 'WEC', \n",
    "    'DWDP', 'PGR', 'MTD', 'NFX', 'UPS'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_results_10 = []\n",
    "\n",
    "# Loop through the top 10 tickers for backtesting\n",
    "for ticker in top_10_tickers:\n",
    "    print(f\"\\nProcessing {ticker}...\")\n",
    "    \n",
    "    # Extract data for the current ticker\n",
    "    data = stock_data_large.get(ticker)\n",
    "    if data is None:\n",
    "        print(f\"No data found for {ticker}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Extract training and testing datasets\n",
    "    X_train, Y_train = data['X_train'], data['Y_train']\n",
    "    X_test, Y_test = data['X_test'], data['Y_test']\n",
    "\n",
    "    # Train the Logistic Regression model using GridSearchCV\n",
    "    print(f\"Training Logistic Regression with GridSearchCV for {ticker}...\")\n",
    "    try:\n",
    "        lr_model = train_lr_model(X_train, Y_train)  # Call the optimized training function\n",
    "\n",
    "        print(f\"Evaluating Logistic Regression for {ticker}...\")\n",
    "        # Copy stock data for manipulation\n",
    "        stock_data_df = data['df'].copy()\n",
    "\n",
    "        # Reset column names, extracting the first level of the MultiIndex\n",
    "        stock_data_df.columns = stock_data_df.columns.get_level_values(0)\n",
    "\n",
    "        # Validate that required columns are present\n",
    "        print(f\"Available columns after reset: {stock_data_df.columns}\")\n",
    "        required_columns = [\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\", \"Date\"]\n",
    "        missing_columns = [col for col in required_columns if col not in stock_data_df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns for {ticker}: {missing_columns}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Slice the test set data\n",
    "        stock_data_test = stock_data_df.iloc[-len(Y_test):].copy()\n",
    "\n",
    "        # Ensure the 'Date' column exists and is of datetime type\n",
    "        if 'Date' in stock_data_test.columns:\n",
    "            stock_data_test['Date'] = pd.to_datetime(stock_data_test['Date'])\n",
    "            stock_data_test.set_index('Date', inplace=True)\n",
    "        else:\n",
    "            print(f\"Date column missing in {ticker}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Ensure the index is a DatetimeIndex\n",
    "        if not isinstance(stock_data_test.index, pd.DatetimeIndex):\n",
    "            print(f\"Invalid index type for {ticker}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Add prediction column to the test set\n",
    "        stock_data_test.loc[:, \"pred\"] = lr_model.predict(X_test)\n",
    "\n",
    "        # Prepare the data for backtesting\n",
    "        bt_data = CustomPandasData(dataname=stock_data_test)\n",
    "\n",
    "        # Run the backtest\n",
    "        sharpe_lr, max_dd_lr = run_backtest(bt_data, ticker, StrategyImplement, \"Logistic Regression\")\n",
    "\n",
    "        # Save backtest results\n",
    "        if sharpe_lr is not None and max_dd_lr is not None:\n",
    "            lr_results_10.append({\"Ticker\": ticker, \"Sharpe_Ratio\": sharpe_lr, \"Max_Drawdown\": max_dd_lr})\n",
    "            print(f\"{ticker} - Logistic Regression Strategy: Sharpe Ratio = {sharpe_lr:.4f}, Max Drawdown = {max_dd_lr:.4f}\")\n",
    "        else:\n",
    "            print(f\"Error generating report for {ticker}: Sharpe Ratio or Max Drawdown is None\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Logistic Regression for {ticker}: {e}\")\n",
    "\n",
    "# Print all backtest results\n",
    "print(\"\\nLogistic Regression Backtest Results for Best Tickers:\")\n",
    "for result in lr_results_10:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_results_10 = []\n",
    "\n",
    "# Loop through the top 10 tickers for backtesting\n",
    "for ticker in top_10_tickers:\n",
    "    print(f\"\\nProcessing {ticker}...\")\n",
    "    \n",
    "    # Extract data for the current ticker\n",
    "    data = stock_data_large.get(ticker)\n",
    "    if data is None:\n",
    "        print(f\"No data found for {ticker}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Extract training and testing datasets\n",
    "    X_train, Y_train = data['X_train'], data['Y_train']\n",
    "    X_test, Y_test = data['X_test'], data['Y_test']\n",
    "\n",
    "    # Train the Random Forest model using GridSearchCV\n",
    "    print(f\"Training Random Forest with GridSearchCV for {ticker}...\")\n",
    "    try:\n",
    "        lr_model = train_rf_model(X_train, Y_train)  # Call the optimized training function for Random Forest\n",
    "\n",
    "        print(f\"Evaluating Random Forest for {ticker}...\")\n",
    "        # Copy stock data for manipulation\n",
    "        stock_data_df = data['df'].copy()\n",
    "\n",
    "        # Reset column names, extracting the first level of the MultiIndex\n",
    "        stock_data_df.columns = stock_data_df.columns.get_level_values(0)\n",
    "\n",
    "        # Validate that required columns are present\n",
    "        print(f\"Available columns after reset: {stock_data_df.columns}\")\n",
    "        required_columns = [\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\", \"Date\"]\n",
    "        missing_columns = [col for col in required_columns if col not in stock_data_df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns for {ticker}: {missing_columns}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Slice the test set data\n",
    "        stock_data_test = stock_data_df.iloc[-len(Y_test):].copy()\n",
    "\n",
    "        # Ensure the 'Date' column exists and is of datetime type\n",
    "        if 'Date' in stock_data_test.columns:\n",
    "            stock_data_test['Date'] = pd.to_datetime(stock_data_test['Date'])\n",
    "            stock_data_test.set_index('Date', inplace=True)\n",
    "        else:\n",
    "            print(f\"Date column missing in {ticker}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Ensure the index is a DatetimeIndex\n",
    "        if not isinstance(stock_data_test.index, pd.DatetimeIndex):\n",
    "            print(f\"Invalid index type for {ticker}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Add prediction column to the test set\n",
    "        stock_data_test.loc[:, \"pred\"] = lr_model.predict(X_test)\n",
    "\n",
    "        # Prepare the data for backtesting\n",
    "        bt_data = CustomPandasData(dataname=stock_data_test)\n",
    "\n",
    "        # Run the backtest\n",
    "        sharpe_rf, max_dd_rf = run_backtest(bt_data, ticker, StrategyImplement, \"Random Forest\")\n",
    "\n",
    "        # Save backtest results\n",
    "        if sharpe_rf is not None and max_dd_rf is not None:\n",
    "            rf_results_10.append({\"Ticker\": ticker, \"Sharpe_Ratio\": sharpe_lr, \"Max_Drawdown\": max_dd_lr})\n",
    "            print(f\"{ticker} - Random Forest Strategy: Sharpe Ratio = {sharpe_lr:.4f}, Max Drawdown = {max_dd_lr:.4f}\")\n",
    "        else:\n",
    "            print(f\"Error generating report for {ticker}: Sharpe Ratio or Max Drawdown is None\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Random Forest for {ticker}: {e}\")\n",
    "\n",
    "# Print all backtest results\n",
    "print(\"\\nRandom Forest Backtest Results for Best Tickers:\")\n",
    "for result in rf_results_10:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to a DataFrame\n",
    "lr_rank = pd.DataFrame(lr_results_10)\n",
    "\n",
    "# Add rankings for each metric\n",
    "lr_rank['Sharpe_Rank'] = lr_rank['Sharpe_Ratio'].rank(ascending=False).astype(int)  # Higher is better\n",
    "lr_rank['Max_Drawdown_Rank'] = lr_rank['Max_Drawdown'].rank(ascending=True).astype(int)  # Less negative is better\n",
    "\n",
    "# Sort the DataFrame by Sharpe_Rank and Max_Drawdown_Rank\n",
    "lr_rank_sorted_sharpe = lr_rank.sort_values(by=\"Sharpe_Rank\")\n",
    "lr_rank_sorted_drawdown = lr_rank.sort_values(by=\"Max_Drawdown_Rank\")\n",
    "\n",
    "# Print ranked results\n",
    "print(\"\\nRanked Stocks by Sharpe Ratio:\")\n",
    "print(lr_rank_sorted_sharpe[['Ticker', 'Sharpe_Ratio', 'Sharpe_Rank']])\n",
    "\n",
    "print(\"\\nRanked Stocks by Max Drawdown:\")\n",
    "print(lr_rank_sorted_drawdown[['Ticker', 'Max_Drawdown', 'Max_Drawdown_Rank']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
